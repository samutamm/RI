{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "from Document import Document\n",
    "from ParserCACM import ParserCACM\n",
    "from ParserQuery import QueryParser, Query\n",
    "from porter import stem\n",
    "from TextRepresenter import PorterStemmer\n",
    "from Index import Index, InvertedIndexPlaces\n",
    "from Weighter import *\n",
    "from IRModel import Vectoriel, LanguageModel, BM25Model, LinearMetaModel\n",
    "from Featurer import Featurer\n",
    "\n",
    "from Evaluation import IRList, PrecisionRecallEval\n",
    "from EvalIRModel import EvalIRModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modèles étudiés :__  \n",
    "Vectoriel weighter 1 (booléen)  \n",
    "Vectoriel weighter 2 (tf)  \n",
    "Vectoriel weighter 3 (tf doc, idf query)  \n",
    "Language Model (weighters 2)  \n",
    "BM25 (weighter 2)  \n",
    "Metamodel  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration des modèles\n",
    "### Historique GridSearch des meilleurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lang_results_cacm_w2.pickle', 'rb') as f:\n",
    "    lang_cacm_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lang_results_cisi_w2.pickle', 'rb') as f:\n",
    "    lang_cisi_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/bm25_results_cacm_w2.pickle', 'rb') as f:\n",
    "    bm25_cacm_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/bm25_results_cisi_w2.pickle', 'rb') as f:\n",
    "    bm25_cisi_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_values = np.linspace(0,1,100)\n",
    "plt.figure()\n",
    "lambda_values = np.geomspace(1e-5, 1, 100)\n",
    "plt.title(\"Modéle de langue − CACM − GridSearch\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel('Mean precision')\n",
    "plt.plot(lambda_values, lang_cacm_results[:,1])\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.xticks(lambda_values, rotation='vertical');\n",
    "\n",
    "\n",
    "idx = np.argmax(lang_cacm_results[:, 1])\n",
    "print(\"Meilleur lambda :\", lang_cacm_results[idx, 0]['lambd'], ', précision moyenne :', lang_cacm_results[idx, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda_values = np.linspace(0,1,100)\n",
    "plt.figure()\n",
    "lambda_values = np.geomspace(1e-5, 1, 100)\n",
    "plt.title(\"Modèle de langue − CISI − GridSearch\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel('Mean precision')\n",
    "plt.plot(lambda_values, lang_cisi_results[:,1])\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.xticks(lambda_values, rotation='vertical');\n",
    "\n",
    "\n",
    "idx = np.argmax(lang_cisi_results[:, 1])\n",
    "print(\"Meilleur lambda :\", lang_cisi_results[idx, 0]['lambd'], ', précision moyenne :', lang_cisi_results[idx, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_a_values = np.linspace(1, 2, 20)\n",
    "param_b_values = list(np.linspace(0.5, 1, 20))\n",
    "bm25_matrix = np.array(bm25_cacm_results[:, 1].reshape(20, 20), dtype=np.float)\n",
    "# k1 : ligne, b : colonnes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(bm25_matrix)\n",
    "plt.xlabel(\"b\")\n",
    "plt.ylabel('k1')\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(param_b_values)))\n",
    "ax.set_yticks(np.arange(len(param_a_values)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(['{:.2f}'.format(k1v) for k1v in param_b_values])\n",
    "ax.set_yticklabels(['{:.2f}'.format(k1v) for k1v in param_a_values])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "ax.set_title(\"bm25 accuracy\")\n",
    "#fmt = StrMethodFormatter('{x}')\n",
    "#ax.yaxis.set_major_formatter(fmt)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "idx = np.argmax(bm25_matrix)\n",
    "print(\"Meilleur k1 :\", param_a_values[idx//20], \", meilleur b :\", param_b_values[idx%20],\n",
    "      \", précision moyenne :\", np.max(bm25_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_a_values = np.linspace(1, 2, 20)\n",
    "param_b_values = list(np.linspace(0.5, 1, 20))\n",
    "bm25_matrix = np.array(bm25_cisi_results[:, 1].reshape(20, 20), dtype=np.float)\n",
    "# k1 : ligne, b : colonnes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(bm25_matrix)\n",
    "plt.xlabel(\"b\")\n",
    "plt.ylabel('k1')\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(param_b_values)))\n",
    "ax.set_yticks(np.arange(len(param_a_values)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(['{:.2f}'.format(k1v) for k1v in param_b_values])\n",
    "ax.set_yticklabels(['{:.2f}'.format(k1v) for k1v in param_a_values])\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "ax.set_title(\"bm25 accuracy\")\n",
    "#fmt = StrMethodFormatter('{x}')\n",
    "#ax.yaxis.set_major_formatter(fmt)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "idx = np.argmax(bm25_matrix)\n",
    "print(\"Meilleur k1 :\", param_a_values[idx//20], \", meilleur b :\", param_b_values[idx%20],\n",
    "      \", précision moyenne :\", np.max(bm25_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des modèles sur CACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index(\"cacm\", \"cacm/cacm.txt\")\n",
    "#index.indexation()\n",
    "\n",
    "weighter1 = WeighterBoolean(index)\n",
    "weighter2 = WeighterVector(index)\n",
    "weighter3 = WeighterSchema3(index)\n",
    "\n",
    "\n",
    "models = {}\n",
    "models[\"vectoriel_w1\"] = Vectoriel(weighter1)\n",
    "models[\"vectoriel_w2\"] = Vectoriel(weighter2)\n",
    "models['vectoriel_w3'] = Vectoriel(weighter3)\n",
    "models['lang'] = LanguageModel(weighter2)\n",
    "models[\"bm25\"] = BM25Model(weighter2)\n",
    "models[\"meta\"] = LinearMetaModel(Featurer(index))\n",
    "model_names = list(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang \n",
    "lambd = 1\n",
    "# bm25\n",
    "k1 = 2\n",
    "b = 0.95\n",
    "# LinearMetaModel\n",
    "models[\"meta\"].load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation sur CACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.8\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results = {}\n",
    "\n",
    "train_results['vectoriel_w1'] = EvalIRModel().evalModel(models['vectoriel_w1'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='train')\n",
    "train_results['vectoriel_w2'] = EvalIRModel().evalModel(models['vectoriel_w2'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='train')\n",
    "train_results['vectoriel_w3'] = EvalIRModel().evalModel(models['vectoriel_w3'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='train')\n",
    "\n",
    "train_results['lang'] = EvalIRModel().evalModel(models['lang'],\n",
    "                                               ranking_call = lambda m, q: m.getRanking(q, lambd=lambd),\n",
    "                                               train_prop=train_prop, seed=seed, mode='train')\n",
    "train_results['bm25'] = EvalIRModel().evalModel(models[\"bm25\"],\n",
    "                                                ranking_call = lambda m,q : m.getRanking(q, k1=k1, b=b),\n",
    "                                               train_prop=train_prop, seed=seed, mode='train')\n",
    "train_results['meta'] = EvalIRModel().evalModel(models[\"meta\"], train_prop=train_prop, seed=seed, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "\n",
    "test_results['vectoriel_w1'] = EvalIRModel().evalModel(models['vectoriel_w1'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='test')\n",
    "test_results['vectoriel_w2'] = EvalIRModel().evalModel(models['vectoriel_w2'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='test')\n",
    "test_results['vectoriel_w3'] = EvalIRModel().evalModel(models['vectoriel_w3'],\n",
    "                                                     train_prop=train_prop, seed=seed, mode='test')\n",
    "\n",
    "test_results['lang'] = EvalIRModel().evalModel(models['lang'],\n",
    "                                               ranking_call = lambda m, q: m.getRanking(q, lambd=lambd),\n",
    "                                               train_prop=train_prop, seed=seed, mode='test')\n",
    "test_results['bm25'] = EvalIRModel().evalModel(models[\"bm25\"],\n",
    "                                                ranking_call = lambda m,q : m.getRanking(q, k1=k1, b=b),\n",
    "                                               train_prop=train_prop, seed=seed, mode='test')\n",
    "test_results['meta'] = EvalIRModel().evalModel(models[\"meta\"], train_prop=train_prop, seed=seed, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "legend = []\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "plt.title(\"20 niveaux de rappels en train\")\n",
    "for model_name, model_results in train_results.items():\n",
    "    legend.append(model_name)\n",
    "    plt.errorbar(range(20), model_results['precision_recall'], yerr=model_results['precision_recall_std'], capsize=4)\n",
    "\n",
    "plt.xlabel(\"Niveau de rappel\")\n",
    "plt.ylabel(\"Précision\")\n",
    "    \n",
    "ax2 = fig.add_subplot(1, 2, 2, sharey=ax1)\n",
    "plt.title('20 niveaux de rappels en test')\n",
    "for model_name, model_results in test_results.items():\n",
    "    ax2.errorbar(range(20), model_results['precision_recall'], yerr=model_results['precision_recall_std'], capsize=4)\n",
    "  \n",
    "    \n",
    "plt.legend(legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "legend = []\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "plt.title('En train')\n",
    "i = 0\n",
    "for model_name, model_results in train_results.items():\n",
    "    legend.append(model_name)\n",
    "    i += 1\n",
    "    ax1.bar(i, model_results['precision_mean'], yerr=model_results['precision_mean_std'], capsize=4)\n",
    "    \n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean precision')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, sharey=ax1)\n",
    "plt.title('En test')\n",
    "i = 0\n",
    "for model_name, model_results in test_results.items():\n",
    "    i += 1\n",
    "    ax2.bar(i, model_results['precision_mean'], yerr=model_results['precision_mean_std'], capsize=4)\n",
    "\n",
    "fig.legend(legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that Meta model does not perform very vell compared to BM25 model. In the training of the metamodel, we did not consider the evaluation scores (only the loss that do depelop nicely), so the problem might be overfitting. Results are consistent between precision−recall and average precision. There is probably mistakes in our language and meta models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
